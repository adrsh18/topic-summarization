{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded  271644  tweets\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_name = 'delta'\n",
    "\n",
    "f = open(file_name+'-en-index-ready.txt', 'r')\n",
    "data = json.loads(f.read())\n",
    "f.close()\n",
    "print \"Loaded \", len(data), \" tweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import gensim\n",
    "from time import time\n",
    "from gensim import corpora, models, similarities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "class TweetTopicExtractor:\n",
    "    def __init__(self, data):\n",
    "        self.raw_data = data\n",
    "        self.documents = None\n",
    "        self.document_corpus = None\n",
    "        self.lda_model = None\n",
    "        self.doc2vec_model = None\n",
    "        self.gensim_dict = None\n",
    "    \n",
    "    def __cleanTokenize(self):\n",
    "        tweet_tokenizer = TweetTokenizer()\n",
    "        stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "        for tweet in self.raw_data:\n",
    "            text_arr = tweet_tokenizer.tokenize(tweet[\"tweet_text\"])\n",
    "            text_arr = [word.lower() for word in text_arr if word.lower() not in stopwords and re.search('[a-zA-Z]', word)]\n",
    "            tweet[\"tweet_text\"] = text_arr\n",
    "    \n",
    "    def __prepareDocuments(self):\n",
    "        documents = []\n",
    "        corp = []\n",
    "        LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "        for tweet in self.raw_data:\n",
    "            documents.append(LabeledSentence(tweet[\"tweet_text\"], [tweet[\"tweet_id\"]]))\n",
    "            corp.append(tweet[\"tweet_text\"])\n",
    "        self.documents = documents\n",
    "        self.document_corpus = corp\n",
    "    \n",
    "    def __getBagOfWords(self):\n",
    "        self.gensim_dict = corpora.Dictionary(self.document_corpus)\n",
    "        self.gensim_dict.filter_extremes(no_below=1, no_above=0.8)\n",
    "        return [self.gensim_dict.doc2bow(text) for text in self.document_corpus]\n",
    "    \n",
    "    def preprocess(self):\n",
    "        t_pp = time()\n",
    "        self.__cleanTokenize()\n",
    "        self.__prepareDocuments()\n",
    "        print \"Cleaned and loaded \", len(self.documents), \" documents (tweets) in \", round(time()-t_pp,2),\" seconds\"\n",
    "    \n",
    "    def buildLda(self, num_topics=4, num_passes=10):\n",
    "        t_lda = time()\n",
    "        bow_corpus = self.__getBagOfWords()\n",
    "        self.lda_model = models.LdaModel(bow_corpus, num_topics=num_topics, id2word=self.gensim_dict, update_every=5, chunksize=100010, passes=num_passes)\n",
    "        print \"Built LDA in \", round(time()-t_lda,2),\" seconds\"\n",
    "    \n",
    "    def printLdaSummary(self):\n",
    "        print \"\"\n",
    "        print \"Topics: Words that define the topic + probability with which word contributes to topic\"\n",
    "        print self.lda_model.show_topics()\n",
    "        \n",
    "        print \"\"\n",
    "        print \"Topics: Just words that define the topic\"\n",
    "        '''\n",
    "        topics_matrix = self.lda_model.show_topics(formatted=False, num_words=20)\n",
    "        topics_matrix = np.array(topics_matrix)\n",
    "\n",
    "        topic_words = topics_matrix[:,:,1]\n",
    "        for i in topic_words:\n",
    "            print [str(word) for word in i]\n",
    "            print \"\"\n",
    "        '''\n",
    "        print \"\"\n",
    "        print \"thisObject.lda_model.xxxx() to explore further\"\n",
    "    \n",
    "    def buildDoc2Vec(self, num_passes=10, parallel=True):\n",
    "        cores = 0\n",
    "        if parallel:\n",
    "            cores = multiprocessing.cpu_count()\n",
    "        t_dv = time()\n",
    "        self.doc2vec_model = models.Doc2Vec(size=100, window=10, dm=0, dbow_words=1, workers=cores, min_count=1, negative=5)\n",
    "        self.doc2vec_model.build_vocab(self.documents)\n",
    "        for i in range(num_passes):\n",
    "            self.doc2vec_model.train(self.documents)\n",
    "            if i%10 == 0:\n",
    "                print \"Iteration \",i\n",
    "        print \"\"\n",
    "        print \"Built Doc2Vec in \",round(time()-t_dv,2), \" seconds\"\n",
    "    \n",
    "    def printDoc2VecSummary(self):\n",
    "        print \"\"\n",
    "        print \"Example: A tweet most similar to '#xmas' can be found like this ->\"\n",
    "        print self.doc2vec_model.most_similar(\"#xmas\")\n",
    "        print \"\"\n",
    "        print \"thisObject.doc2vec_model.xxxx() to explore further\"\n",
    "    \n",
    "    def saveLdaModel(self, path_to_lda='cooper.lda'):\n",
    "        self.lda_model.save(path_to_lda)\n",
    "        print \"Saved\"\n",
    "    \n",
    "    def saveDoc2VecModel(self, path_to_doc2vec='cooper.doc2vec'):\n",
    "        self.doc2vec_model.save(path_to_doc2vec)\n",
    "        print \"Saved\"\n",
    "    \n",
    "    def loadLdaModel(self, path_to_lda='cooper.lda'):\n",
    "        self.lda_model = models.LdaModel.load(path_to_lda, mmap='r')\n",
    "        print \"Loaded\"\n",
    "    \n",
    "    def loadDoc2VecModel(self, path_to_doc2vec='cooper.doc2vec'):\n",
    "        self.doc2vec_model = models.Doc2Vec.load(path_to_doc2vec, mmap='r')\n",
    "        print \"Loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and loaded  100001  documents (tweets) in  10.28  seconds\n"
     ]
    }
   ],
   "source": [
    "tte = TweetTopicExtractor(data)\n",
    "tte.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built Doc2Vec in  1735.08  seconds\n",
      "\n",
      "Example: A tweet most similar to '#Xmas' can be found like this ->\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word '#Xmas' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-024f9ebf4c84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_passes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintDoc2VecSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-1dcc1d5fd252>\u001b[0m in \u001b[0;36mprintDoc2VecSummary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Example: A tweet most similar to '#Xmas' can be found like this ->\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mprint\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#Xmas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"thisObject.doc2vec_model.xxxx() to explore further\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/adarsh/.local/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot compute similarity with no input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '#Xmas' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "tte.buildDoc2Vec(num_passes=100)\n",
    "tte.printDoc2VecSummary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built LDA in  2065.32  seconds\n",
      "\n",
      "Topics: Words that define the topic + probability with which word contributes to topic\n",
      "[(0, u'0.048*\"#christmas\" + 0.033*\"#Christmas\" + 0.018*\"Days\" + 0.018*\"Christmas\" + 0.010*\"one\" + 0.009*\"time\" + 0.008*\"Get\" + 0.008*\"I\\'ve\" + 0.007*\"involved\" + 0.007*\"entered\"'), (1, u'0.092*\"#Christmas\" + 0.020*\"December\" + 0.013*\"1st\" + 0.010*\"Christmas\" + 0.010*\"time\" + 0.009*\"It\\'s\" + 0.008*\"Happy\" + 0.008*\"us\" + 0.007*\"The\" + 0.007*\"#December\"'), (2, u'0.089*\"#christmas\" + 0.073*\"#christmascookies\" + 0.073*\"#santa\\'schimney\" + 0.073*\"#sweeper\" + 0.073*\"#christmascrush\" + 0.073*\"#christmasmatch3\" + 0.021*\"#Christmas\" + 0.006*\"Christmas\" + 0.002*\"December\" + 0.002*\"The\"'), (3, u'0.074*\"#christmas\" + 0.015*\"#Christmas\" + 0.012*\"The\" + 0.012*\"Christmas\" + 0.010*\"#Xmas\" + 0.010*\"#sale\" + 0.007*\"#december\" + 0.006*\"one\" + 0.006*\"amazing\" + 0.005*\"Share\"'), (4, u'0.076*\"#christmas\" + 0.036*\"Christmas\" + 0.015*\"#Christmas\" + 0.008*\"#Xmas\" + 0.008*\"#sale\" + 0.006*\"December\" + 0.006*\"#xmas\" + 0.006*\"tree\" + 0.005*\"I\" + 0.005*\"#christmastree\"'), (5, u'0.080*\"#Christmas\" + 0.018*\"Christmas\" + 0.014*\"I\" + 0.012*\"#christmas\" + 0.007*\"The\" + 0.005*\"year\" + 0.004*\"get\" + 0.004*\"great\" + 0.004*\"love\" + 0.004*\"This\"'), (6, u'0.052*\"#Christmas\" + 0.024*\"via\" + 0.015*\"#christmas\" + 0.014*\"@Etsy\" + 0.009*\"Christmas\" + 0.007*\"RT\" + 0.005*\"A\" + 0.005*\"#handmade\" + 0.005*\"Tree\" + 0.004*\"#ChristmasPlease\"'), (7, u'0.056*\"#Christmas\" + 0.026*\"#giveaway\" + 0.019*\"#Win\" + 0.016*\"BIG\" + 0.016*\"@AttachmentMumma\" + 0.010*\"RT\" + 0.009*\"#christmas\" + 0.008*\"#Events\" + 0.008*\"#Tickets\" + 0.007*\"Tickets\"'), (8, u'0.077*\"#Christmas\" + 0.013*\"#christmas\" + 0.011*\"Christmas\" + 0.009*\"gift\" + 0.006*\"The\" + 0.005*\"#gifts\" + 0.004*\"A\" + 0.004*\"Check\" + 0.003*\"advent\" + 0.003*\"I\"'), (9, u'0.062*\"#Christmas\" + 0.033*\"like\" + 0.025*\"Christmas\" + 0.025*\"look\" + 0.019*\"It\\'s\" + 0.018*\"lot\" + 0.017*\"beginning\" + 0.016*\"#christmas\" + 0.009*\"Gift\" + 0.004*\"season\"')]\n",
      "\n",
      "Topics: Just words that define the topic\n",
      "\n",
      "thisObject.lda_model.xxxx() to explore further\n"
     ]
    }
   ],
   "source": [
    "tte.buildLda(num_topics=10)\n",
    "tte.printLdaSummary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "tte.saveModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'know', 0.42546698451042175), (u'amazing', 0.4127165675163269), (u'right', 0.3982900083065033), (u'#disney', 0.33922475576400757), (u'#crafts', 0.3272498846054077), (u'#thanksgiving', 0.3166525661945343), (u'#decorations', 0.30977874994277954), (u'special', 0.3092844784259796), (u'almonds', 0.3067658543586731), (u'excited', 0.30661681294441223)]\n"
     ]
    }
   ],
   "source": [
    "print tte.doc2vec_model.most_similar('#xmas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = TweetTopicExtractor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "x.loadModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and loaded  271644  documents (tweets) in  30.22  seconds\n"
     ]
    }
   ],
   "source": [
    "tte = TweetTopicExtractor(data)\n",
    "tte.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "Iteration  10\n",
      "Iteration  20\n",
      "Iteration  30\n",
      "Iteration  40\n",
      "\n",
      "Built Doc2Vec in  5562.25  seconds\n"
     ]
    }
   ],
   "source": [
    "tte.buildDoc2Vec(num_passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "tte.saveDoc2VecModel('delta.cooper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors loaded:  (271644, 100)  and  (271644,)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "g = open(file_name+'-en-index-ready.txt','r')\n",
    "tweets = json.loads(g.read())\n",
    "g.close()\n",
    "\n",
    "vectors = []\n",
    "labels = []\n",
    "\n",
    "for t in tweets:\n",
    "    v = tte.doc2vec_model.docvecs[t['tweet_id']]\n",
    "    vectors.append(v)\n",
    "    labels.append(t['tweet_id'])\n",
    "vectors = np.asarray(vectors)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "print \"Vectors loaded: \",vectors.shape,\" and \",labels.shape\n",
    "\n",
    "g = open(file_name+'-vectors.pkl','wb')\n",
    "cPickle.dump((vectors,labels),g)\n",
    "g.close()\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
